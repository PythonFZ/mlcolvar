{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DictModule(dataset -> DictDataset( \"data_list\": 28603, \"z_table\": [6, 7, 8], \"cutoff\": 8.0, \"data_type\": graphs ),\n",
      "\t\t     train_loader -> DictLoader(length=1, batch_size=28603, shuffle=False))\n"
     ]
    }
   ],
   "source": [
    "from mlcolvar.data import DictModule\n",
    "from mlcolvar.utils.io import create_dataset_from_trajectories\n",
    "from mlcolvar.utils.io import create_dataset_from_files\n",
    "\n",
    "\n",
    "dataset_graph = create_dataset_from_trajectories(\n",
    "    trajectories=[\n",
    "        '/home/etrizio@iit.local/notebooks/projects/kolmogorov/alanine_transform/unbiased_sims/state_A/traj_comp.xtc',\n",
    "        '/home/etrizio@iit.local/notebooks/projects/kolmogorov/alanine_transform/unbiased_sims/state_B/traj_comp.xtc',\n",
    "        '/home/etrizio@iit.local/notebooks/projects/kolmogorov_opes/alanine_long_train/biased_sims/iter_4/A/traj_comp.xtc',\n",
    "        # '/home/etrizio@iit.local/notebooks/projects/kolmogorov_opes/alanine_long_train/biased_sims/iter_4/B/traj_comp.xtc'\n",
    "        # 'data/r.dcd',\n",
    "        # 'data/p.dcd',\n",
    "        # 'data/biased.trajectory.h5',\n",
    "        # 'data/biased.dcd',\n",
    "        #'data/biased.trajectory.h5',\n",
    "        #'data/r.dcd'\n",
    "    ],\n",
    "    top=['/home/etrizio@iit.local/notebooks/projects/kolmogorov/alanine_transform/unbiased_sims/state_A/confout.gro',\n",
    "         '/home/etrizio@iit.local/notebooks/projects/kolmogorov/alanine_transform/unbiased_sims/state_A/confout.gro',\n",
    "         '/home/etrizio@iit.local/notebooks/projects/kolmogorov/alanine_transform/unbiased_sims/state_A/confout.gro'\n",
    "        # 'data/r.pdb', \n",
    "        #  'data/p.pdb',\n",
    "        #  'data/r.pdb',\n",
    "         #'data/r.pdb'\n",
    "         ],\n",
    "    cutoff=8.0,  # Ang\n",
    "    create_labels=True,\n",
    "    system_selection='all and not type H',\n",
    "    show_progress=False\n",
    ")\n",
    "\n",
    "\n",
    "datamodule_graph = DictModule(dataset_graph, lengths=[1], shuffle=False)\n",
    "print(datamodule_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from mlcolvar.utils.io import load_dataframe\n",
    "\n",
    "T = 300 \n",
    "# Boltzmann factor in the RIGHT ENRGY UNITS!\n",
    "kb = 0.0083144621\n",
    "beta = 1/(kb*T)\n",
    "\n",
    "df = load_dataframe('/home/etrizio@iit.local/notebooks/projects/kolmogorov_opes/alanine_long_train/biased_sims/iter_4/A/COLVAR', stride=1)\n",
    "weights = torch.exp(1/beta*torch.tensor((df['opes.bias'] + df['bias']).values))\n",
    "weights = weights / weights.sum()\n",
    "weights\n",
    "\n",
    "aux = torch.zeros(len(dataset_graph))\n",
    "for i in range(len(weights)):\n",
    "    aux[-(len(weights)-i)] = weights[i]\n",
    "aux\n",
    "\n",
    "for i in range(len(dataset_graph)):\n",
    "    dataset_graph['data_list'][i]['weight'] = aux[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28603"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/etrizio@iit.local/Bin/miniconda3/envs/graph_mlcolvar_test/lib/python3.9/site-packages/lightning/pytorch/utilities/parsing.py:198: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Committor(\n",
       "  (loss_fn): CommittorLoss()\n",
       "  (nn): SchNetModel(\n",
       "    (_radial_embedding): RadialEmbeddingBlock(\n",
       "      (bessel_fn): GAUSSIANBASIS [ \u001b[32m16\u001b[0m\u001b[36m 󰯰 \u001b[0m| \u001b[32m8.000000\u001b[0m\u001b[36m 󰳁 \u001b[0m]\n",
       "    )\n",
       "    (W_v): Linear(in_features=3, out_features=32, bias=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x InteractionBlock(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=16, out_features=32, bias=True)\n",
       "          (1): ShiftedSoftplus()\n",
       "          (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "        (conv): CFConv()\n",
       "        (act): ShiftedSoftplus()\n",
       "        (lin): Linear(in_features=32, out_features=32, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (W_out): ModuleList(\n",
       "      (0): Linear(in_features=32, out_features=16, bias=True)\n",
       "      (1): ShiftedSoftplus()\n",
       "      (2): Linear(in_features=16, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (sigmoid): Custom_Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mlcolvar.cvs.committor import Committor\n",
    "from mlcolvar.core.nn.graph.schnet import SchNetModel\n",
    "import torch\n",
    "\n",
    "gnn_model = SchNetModel(n_out=1,\n",
    "                        cutoff=dataset_graph.metadata['cutoff'],\n",
    "                        atomic_numbers=dataset_graph.metadata['z_table'],\n",
    "                        n_bases=16,\n",
    "                        n_layers=2,\n",
    "                        n_filters=32,\n",
    "                        n_hidden_channels=32,\n",
    "                    )\n",
    "\n",
    "# model = Committor(model=gnn_model,\n",
    "#                   mass=torch.Tensor([12, 19]),\n",
    "#                   alpha=1)\n",
    "\n",
    "\n",
    "model = Committor(model=gnn_model,\n",
    "                  mass=torch.Tensor([12, 14, 16]),\n",
    "                  alpha=1)\n",
    "\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/etrizio@iit.local/Bin/miniconda3/envs/graph_mlcolvar_test/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b16af39d1674ea68b070e8ae805baf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 316.00 MiB. GPU 0 has a total capacty of 7.79 GiB of which 244.31 MiB is free. Including non-PyTorch memory, this process has 6.88 GiB memory in use. Of the allocated memory 6.61 GiB is allocated by PyTorch, and 158.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 13\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlightning\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Trainer\n\u001b[1;32m      3\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m      4\u001b[0m     logger\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m      5\u001b[0m     enable_checkpointing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m     num_sanity_val_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     11\u001b[0m )\n\u001b[0;32m---> 13\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule_graph\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Bin/miniconda3/envs/graph_mlcolvar_test/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py:544\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 544\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Bin/miniconda3/envs/graph_mlcolvar_test/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/Bin/miniconda3/envs/graph_mlcolvar_test/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py:580\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    574\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    575\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    576\u001b[0m     ckpt_path,\n\u001b[1;32m    577\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    578\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    579\u001b[0m )\n\u001b[0;32m--> 580\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/Bin/miniconda3/envs/graph_mlcolvar_test/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py:989\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    986\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    987\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 989\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    991\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    992\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    993\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    994\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Bin/miniconda3/envs/graph_mlcolvar_test/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py:1035\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1033\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_sanity_check()\n\u001b[1;32m   1034\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1035\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected state \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Bin/miniconda3/envs/graph_mlcolvar_test/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:202\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[0;32m--> 202\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/Bin/miniconda3/envs/graph_mlcolvar_test/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:359\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 359\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Bin/miniconda3/envs/graph_mlcolvar_test/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py:136\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone:\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 136\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end(data_fetcher)\n\u001b[1;32m    138\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/Bin/miniconda3/envs/graph_mlcolvar_test/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py:240\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mautomatic_optimization:\n\u001b[1;32m    239\u001b[0m         \u001b[38;5;66;03m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[0;32m--> 240\u001b[0m         batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautomatic_optimization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    242\u001b[0m         batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanual_optimization\u001b[38;5;241m.\u001b[39mrun(kwargs)\n",
      "File \u001b[0;32m~/Bin/miniconda3/envs/graph_mlcolvar_test/lib/python3.9/site-packages/lightning/pytorch/loops/optimization/automatic.py:187\u001b[0m, in \u001b[0;36m_AutomaticOptimization.run\u001b[0;34m(self, optimizer, batch_idx, kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m         closure()\n\u001b[1;32m    182\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;66;03m# BACKWARD PASS\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;66;03m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 187\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m result \u001b[38;5;241m=\u001b[39m closure\u001b[38;5;241m.\u001b[39mconsume_result()\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Bin/miniconda3/envs/graph_mlcolvar_test/lib/python3.9/site-packages/lightning/pytorch/loops/optimization/automatic.py:265\u001b[0m, in \u001b[0;36m_AutomaticOptimization._optimizer_step\u001b[0;34m(self, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim_progress\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep\u001b[38;5;241m.\u001b[39mincrement_ready()\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# model hook\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moptimizer_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_step_and_backward_closure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_accumulate:\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim_progress\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep\u001b[38;5;241m.\u001b[39mincrement_completed()\n",
      "File \u001b[0;32m~/Bin/miniconda3/envs/graph_mlcolvar_test/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py:157\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[0;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m hook_name\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 157\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    160\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/Bin/miniconda3/envs/graph_mlcolvar_test/lib/python3.9/site-packages/lightning/pytorch/core/module.py:1291\u001b[0m, in \u001b[0;36mLightningModule.optimizer_step\u001b[0;34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[0m\n\u001b[1;32m   1252\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimizer_step\u001b[39m(\n\u001b[1;32m   1253\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1254\u001b[0m     epoch: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1257\u001b[0m     optimizer_closure: Optional[Callable[[], Any]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1258\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1259\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Override this method to adjust the default way the :class:`~lightning.pytorch.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[1;32m   1260\u001b[0m \u001b[38;5;124;03m    the optimizer.\u001b[39;00m\n\u001b[1;32m   1261\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1289\u001b[0m \n\u001b[1;32m   1290\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1291\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_closure\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Bin/miniconda3/envs/graph_mlcolvar_test/lib/python3.9/site-packages/lightning/pytorch/core/optimizer.py:151\u001b[0m, in \u001b[0;36mLightningOptimizer.step\u001b[0;34m(self, closure, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen `optimizer.step(closure)` is called, the closure should be callable\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strategy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 151\u001b[0m step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_after_step()\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m step_output\n",
      "File \u001b[0;32m~/Bin/miniconda3/envs/graph_mlcolvar_test/lib/python3.9/site-packages/lightning/pytorch/strategies/strategy.py:230\u001b[0m, in \u001b[0;36mStrategy.optimizer_step\u001b[0;34m(self, optimizer, closure, model, **kwargs)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;66;03m# TODO(fabric): remove assertion once strategy's optimizer_step typing is fixed\u001b[39;00m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, pl\u001b[38;5;241m.\u001b[39mLightningModule)\n\u001b[0;32m--> 230\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecision_plugin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Bin/miniconda3/envs/graph_mlcolvar_test/lib/python3.9/site-packages/lightning/pytorch/plugins/precision/precision.py:117\u001b[0m, in \u001b[0;36mPrecision.optimizer_step\u001b[0;34m(self, optimizer, model, closure, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Hook to run the optimizer step.\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m closure \u001b[38;5;241m=\u001b[39m partial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_closure, model, optimizer, closure)\n\u001b[0;32m--> 117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Bin/miniconda3/envs/graph_mlcolvar_test/lib/python3.9/site-packages/torch/optim/optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[0;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/Bin/miniconda3/envs/graph_mlcolvar_test/lib/python3.9/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/Bin/miniconda3/envs/graph_mlcolvar_test/lib/python3.9/site-packages/torch/optim/adam.py:143\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m closure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[0;32m--> 143\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups:\n\u001b[1;32m    146\u001b[0m     params_with_grad \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/Bin/miniconda3/envs/graph_mlcolvar_test/lib/python3.9/site-packages/lightning/pytorch/plugins/precision/precision.py:104\u001b[0m, in \u001b[0;36mPrecision._wrap_closure\u001b[0;34m(self, model, optimizer, closure)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrap_closure\u001b[39m(\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     93\u001b[0m     model: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.LightningModule\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     94\u001b[0m     optimizer: Optimizer,\n\u001b[1;32m     95\u001b[0m     closure: Callable[[], Any],\n\u001b[1;32m     96\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m     97\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the ``on_before_optimizer_step``\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    hook is called.\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    102\u001b[0m \n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 104\u001b[0m     closure_result \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_after_closure(model, optimizer)\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m closure_result\n",
      "File \u001b[0;32m~/Bin/miniconda3/envs/graph_mlcolvar_test/lib/python3.9/site-packages/lightning/pytorch/loops/optimization/automatic.py:140\u001b[0m, in \u001b[0;36mClosure.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Tensor]:\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\u001b[38;5;241m.\u001b[39mloss\n",
      "File \u001b[0;32m~/Bin/miniconda3/envs/graph_mlcolvar_test/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Bin/miniconda3/envs/graph_mlcolvar_test/lib/python3.9/site-packages/lightning/pytorch/loops/optimization/automatic.py:126\u001b[0m, in \u001b[0;36mClosure.closure\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39menable_grad()\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclosure\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ClosureResult:\n\u001b[0;32m--> 126\u001b[0m     step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m step_output\u001b[38;5;241m.\u001b[39mclosure_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarning_cache\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`training_step` returned `None`. If this was on purpose, ignore this warning...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Bin/miniconda3/envs/graph_mlcolvar_test/lib/python3.9/site-packages/lightning/pytorch/loops/optimization/automatic.py:315\u001b[0m, in \u001b[0;36m_AutomaticOptimization._training_step\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m    312\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\n\u001b[1;32m    314\u001b[0m \u001b[38;5;66;03m# manually capture logged metrics\u001b[39;00m\n\u001b[0;32m--> 315\u001b[0m training_step_output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mpost_training_step()  \u001b[38;5;66;03m# unused hook - call anyway for backward compatibility\u001b[39;00m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_result_cls\u001b[38;5;241m.\u001b[39mfrom_training_step_output(training_step_output, trainer\u001b[38;5;241m.\u001b[39maccumulate_grad_batches)\n",
      "File \u001b[0;32m~/Bin/miniconda3/envs/graph_mlcolvar_test/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py:309\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 309\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    312\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/Bin/miniconda3/envs/graph_mlcolvar_test/lib/python3.9/site-packages/lightning/pytorch/strategies/strategy.py:382\u001b[0m, in \u001b[0;36mStrategy.training_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module:\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 382\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Bin/dev/mlcolvar/mlcolvar/cvs/committor/committor.py:136\u001b[0m, in \u001b[0;36mCommittor.training_step\u001b[0;34m(self, train_batch, batch_idx)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;66;03m# ===================loss=====================\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[0;32m--> 136\u001b[0m     loss, loss_var, loss_bound_A, loss_bound_B \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    140\u001b[0m     loss, loss_var, loss_bound_A, loss_bound_B \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn(\n\u001b[1;32m    141\u001b[0m         x, q, labels, weights \n\u001b[1;32m    142\u001b[0m     )\n",
      "File \u001b[0;32m~/Bin/miniconda3/envs/graph_mlcolvar_test/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Bin/miniconda3/envs/graph_mlcolvar_test/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Bin/dev/mlcolvar/mlcolvar/core/loss/committor_loss.py:77\u001b[0m, in \u001b[0;36mCommittorLoss.forward\u001b[0;34m(self, x, q, labels, w, create_graph)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \n\u001b[1;32m     71\u001b[0m             x: Union[torch\u001b[38;5;241m.\u001b[39mTensor, torch_geometric\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mBatch], \n\u001b[1;32m     72\u001b[0m             q: torch\u001b[38;5;241m.\u001b[39mTensor, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     75\u001b[0m             create_graph: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     76\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m---> 77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcommittor_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m                            \u001b[49m\u001b[43matomic_masses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43matomic_masses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m                            \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mdelta_f\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdelta_f\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mseparate_boundary_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseparate_boundary_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mdescriptors_derivatives\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdescriptors_derivatives\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mlog_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_var\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m                        \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Bin/dev/mlcolvar/mlcolvar/core/loss/committor_loss.py:207\u001b[0m, in \u001b[0;36mcommittor_loss\u001b[0;34m(x, q, labels, w, atomic_masses, alpha, gamma, delta_f, create_graph, separate_boundary_dataset, descriptors_derivatives, log_var)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;66;03m# ==============================  LOSS ==============================\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;66;03m# Each loss contribution is scaled by the number of samples\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \n\u001b[1;32m    204\u001b[0m \u001b[38;5;66;03m# 1. VARIATIONAL LOSS\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;66;03m# Compute gradients of q(x) wrt x\u001b[39;00m\n\u001b[1;32m    206\u001b[0m grad_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones_like(q[mask_var])\n\u001b[0;32m--> 207\u001b[0m grad \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmask_var\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_graph\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    208\u001b[0m grad \u001b[38;5;241m=\u001b[39m grad[mask_var_batches]\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m descriptors_derivatives \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;66;03m# we use the precomputed derivatives from descriptors to pos\u001b[39;00m\n",
      "File \u001b[0;32m~/Bin/miniconda3/envs/graph_mlcolvar_test/lib/python3.9/site-packages/torch/autograd/__init__.py:394\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[1;32m    390\u001b[0m     result \u001b[38;5;241m=\u001b[39m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[1;32m    391\u001b[0m         grad_outputs_\n\u001b[1;32m    392\u001b[0m     )\n\u001b[1;32m    393\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 394\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    395\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[1;32m    404\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[1;32m    405\u001b[0m         output\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    407\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mzeros_like(\u001b[38;5;28minput\u001b[39m, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    408\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m (output, \u001b[38;5;28minput\u001b[39m) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(result, t_inputs)\n\u001b[1;32m    409\u001b[0m     )\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 316.00 MiB. GPU 0 has a total capacty of 7.79 GiB of which 244.31 MiB is free. Including non-PyTorch memory, this process has 6.88 GiB memory in use. Of the allocated memory 6.61 GiB is allocated by PyTorch, and 158.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "from lightning import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    logger=False,\n",
    "    enable_checkpointing=False,\n",
    "    accelerator='cuda',\n",
    "    max_epochs=500,\n",
    "    enable_model_summary=False,\n",
    "    limit_val_batches=0, \n",
    "    num_sanity_val_steps=0\n",
    ")\n",
    "\n",
    "trainer.fit(model, datamodule_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = datamodule_graph\n",
    "test = next(iter(loader.train_dataloader()))['data_list']\n",
    "out_graph = model(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.1561e-08],\n",
       "        [1.6096e-09],\n",
       "        [1.0000e+00],\n",
       "        ...,\n",
       "        [1.0000e+00],\n",
       "        [1.3562e-09],\n",
       "        [1.0000e+00]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.hist(out_graph.detach().squeeze())\n",
    "plt.plot(out_graph.detach().squeeze(), '.')\n",
    "plt.ylim(0,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graph_mlcolvar_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
